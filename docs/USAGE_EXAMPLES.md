# GraphRAG çŸ¥è¯†åº“ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº† GraphRAG çŸ¥è¯†åº“ç³»ç»Ÿçš„è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬å¸¸è§ä½¿ç”¨åœºæ™¯ã€å®Œæ•´çš„å·¥ä½œæµç¨‹å’Œæœ€ä½³å®è·µã€‚é€šè¿‡è¿™äº›ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿä¸Šæ‰‹å¹¶å……åˆ†åˆ©ç”¨ç³»ç»Ÿçš„å„é¡¹åŠŸèƒ½ã€‚

## ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ–‡æ¡£ç®¡ç†ç¤ºä¾‹](#æ–‡æ¡£ç®¡ç†ç¤ºä¾‹)
3. [çŸ¥è¯†æŠ½å–ç¤ºä¾‹](#çŸ¥è¯†æŠ½å–ç¤ºä¾‹)
4. [å›¾æŸ¥è¯¢ç¤ºä¾‹](#å›¾æŸ¥è¯¢ç¤ºä¾‹)
5. [RAG é—®ç­”ç¤ºä¾‹](#rag-é—®ç­”ç¤ºä¾‹)
6. [å®Œæ•´å·¥ä½œæµç¨‹](#å®Œæ•´å·¥ä½œæµç¨‹)
7. [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)
8. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨ç³»ç»Ÿ

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
./scripts/start.sh

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/api/v1/system/health
```

### 2. ä¸Šä¼ ç¬¬ä¸€ä¸ªæ–‡æ¡£

```bash
# ä¸Šä¼  PDF æ–‡æ¡£
curl -X POST "http://localhost:8000/api/v1/document-management/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "files=@research_paper.pdf" \
  -F "metadata={\"category\": \"research\", \"tags\": [\"AI\", \"ML\"]}"
```

### 3. å¤„ç†æ–‡æ¡£

```bash
# å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£
curl -X POST "http://localhost:8000/api/v1/document-management/process/doc_123" \
  -H "Content-Type: application/json" \
  -d '{
    "processing_options": {
      "extract_text": true,
      "create_chunks": true,
      "generate_embeddings": true,
      "extract_entities": true
    }
  }'
```

### 4. è¿›è¡Œé—®ç­”

```bash
# åŸºäºçŸ¥è¯†åº“é—®ç­”
curl -X POST "http://localhost:8000/api/v1/rag/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ",
    "include_sources": true
  }'
```

## æ–‡æ¡£ç®¡ç†ç¤ºä¾‹

### åœºæ™¯1ï¼šæ‰¹é‡ä¸Šä¼ å­¦æœ¯è®ºæ–‡

```python
import requests
import os
from pathlib import Path

def upload_papers_batch(papers_dir: str, category: str = "research"):
    """æ‰¹é‡ä¸Šä¼ å­¦æœ¯è®ºæ–‡"""
    
    base_url = "http://localhost:8000/api/v1"
    upload_url = f"{base_url}/document-management/upload"
    
    papers_path = Path(papers_dir)
    pdf_files = list(papers_path.glob("*.pdf"))
    
    print(f"æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
    
    # æ‰¹é‡ä¸Šä¼ ï¼ˆæ¯æ¬¡æœ€å¤š5ä¸ªæ–‡ä»¶ï¼‰
    batch_size = 5
    for i in range(0, len(pdf_files), batch_size):
        batch_files = pdf_files[i:i+batch_size]
        
        files = []
        for pdf_file in batch_files:
            files.append(('files', (pdf_file.name, open(pdf_file, 'rb'), 'application/pdf')))
        
        metadata = {
            "category": category,
            "tags": ["academic", "research"],
            "batch_id": f"batch_{i//batch_size + 1}"
        }
        
        data = {
            "metadata": json.dumps(metadata)
        }
        
        try:
            response = requests.post(upload_url, files=files, data=data)
            response.raise_for_status()
            
            result = response.json()
            print(f"æ‰¹æ¬¡ {i//batch_size + 1} ä¸Šä¼ æˆåŠŸ:")
            for file_info in result["data"]["uploaded_files"]:
                print(f"  - {file_info['filename']}: {file_info['id']}")
                
        except requests.exceptions.RequestException as e:
            print(f"æ‰¹æ¬¡ {i//batch_size + 1} ä¸Šä¼ å¤±è´¥: {e}")
        
        finally:
            # å…³é—­æ–‡ä»¶
            for _, (_, file_obj, _) in files:
                file_obj.close()

# ä½¿ç”¨ç¤ºä¾‹
upload_papers_batch("./papers", "ai_research")
```

### åœºæ™¯2ï¼šç›‘æ§æ–‡æ¡£å¤„ç†çŠ¶æ€

```python
import time
import requests

def monitor_document_processing(document_id: str):
    """ç›‘æ§æ–‡æ¡£å¤„ç†çŠ¶æ€"""
    
    base_url = "http://localhost:8000/api/v1"
    status_url = f"{base_url}/document-management/documents/{document_id}"
    
    print(f"å¼€å§‹ç›‘æ§æ–‡æ¡£ {document_id} çš„å¤„ç†çŠ¶æ€...")
    
    while True:
        try:
            response = requests.get(status_url)
            response.raise_for_status()
            
            doc_info = response.json()["data"]
            status = doc_info["status"]
            
            print(f"å½“å‰çŠ¶æ€: {status}")
            
            if status == "processed":
                print("âœ… æ–‡æ¡£å¤„ç†å®Œæˆ!")
                print(f"  - æ–‡æœ¬å—æ•°é‡: {len(doc_info.get('chunks', []))}")
                print(f"  - å®ä½“æ•°é‡: {len(doc_info.get('entities', []))}")
                break
            elif status == "failed":
                print("âŒ æ–‡æ¡£å¤„ç†å¤±è´¥!")
                print(f"  - é”™è¯¯ä¿¡æ¯: {doc_info.get('error_message', 'Unknown error')}")
                break
            elif status in ["uploaded", "processing"]:
                print("â³ å¤„ç†ä¸­ï¼Œç­‰å¾…5ç§’åé‡è¯•...")
                time.sleep(5)
            else:
                print(f"âš ï¸  æœªçŸ¥çŠ¶æ€: {status}")
                break
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            break
        except KeyboardInterrupt:
            print("\nâš ï¸  ç›‘æ§è¢«ç”¨æˆ·ä¸­æ–­")
            break

# ä½¿ç”¨ç¤ºä¾‹
monitor_document_processing("doc_123")
```

### åœºæ™¯3ï¼šæ–‡æ¡£å…ƒæ•°æ®ç®¡ç†

```python
def update_document_metadata(document_id: str, new_metadata: dict):
    """æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®"""
    
    base_url = "http://localhost:8000/api/v1"
    update_url = f"{base_url}/document-management/documents/{document_id}"
    
    payload = {
        "metadata": new_metadata,
        "update_mode": "merge"  # åˆå¹¶æ¨¡å¼ï¼Œä¿ç•™åŸæœ‰å…ƒæ•°æ®
    }
    
    try:
        response = requests.patch(update_url, json=payload)
        response.raise_for_status()
        
        result = response.json()
        print("âœ… å…ƒæ•°æ®æ›´æ–°æˆåŠŸ!")
        print(f"æ›´æ–°åçš„å…ƒæ•°æ®: {result['data']['metadata']}")
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ å…ƒæ•°æ®æ›´æ–°å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
new_metadata = {
    "authors": ["å¼ ä¸‰", "æå››"],
    "publication_year": 2024,
    "journal": "AI Research Journal",
    "keywords": ["æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", "çŸ¥è¯†å›¾è°±"],
    "abstract": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•..."
}

update_document_metadata("doc_123", new_metadata)
```

## çŸ¥è¯†æŠ½å–ç¤ºä¾‹

### åœºæ™¯1ï¼šè‡ªå®šä¹‰å®ä½“æŠ½å–

```python
def extract_custom_entities(text: str, custom_types: list):
    """è‡ªå®šä¹‰å®ä½“æŠ½å–"""
    
    base_url = "http://localhost:8000/api/v1"
    extract_url = f"{base_url}/knowledge/extract-entities"
    
    payload = {
        "text": text,
        "entity_types": custom_types,
        "language": "zh",
        "confidence_threshold": 0.7,
        "custom_patterns": [
            {
                "type": "MODEL_NAME",
                "pattern": r"(GPT-\d+|BERT|Transformer|ResNet-\d+)",
                "description": "AIæ¨¡å‹åç§°"
            },
            {
                "type": "METRIC",
                "pattern": r"(å‡†ç¡®ç‡|å¬å›ç‡|F1åˆ†æ•°|BLEUåˆ†æ•°)",
                "description": "è¯„ä¼°æŒ‡æ ‡"
            }
        ]
    }
    
    try:
        response = requests.post(extract_url, json=payload)
        response.raise_for_status()
        
        result = response.json()["data"]
        entities = result["entities"]
        
        print(f"âœ… æŠ½å–åˆ° {len(entities)} ä¸ªå®ä½“:")
        
        # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º
        entities_by_type = {}
        for entity in entities:
            entity_type = entity["type"]
            if entity_type not in entities_by_type:
                entities_by_type[entity_type] = []
            entities_by_type[entity_type].append(entity)
        
        for entity_type, type_entities in entities_by_type.items():
            print(f"\n{entity_type}:")
            for entity in type_entities:
                print(f"  - {entity['name']} (ç½®ä¿¡åº¦: {entity['confidence']:.2f})")
                if entity.get('properties'):
                    for key, value in entity['properties'].items():
                        print(f"    {key}: {value}")
        
        return entities
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ å®ä½“æŠ½å–å¤±è´¥: {e}")
        return []

# ä½¿ç”¨ç¤ºä¾‹
text = """
æœ¬ç ”ç©¶ä½¿ç”¨GPT-4æ¨¡å‹è¿›è¡Œè‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ï¼Œåœ¨WMTæ•°æ®é›†ä¸Šå–å¾—äº†95%çš„å‡†ç¡®ç‡ã€‚
æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†BERTå’ŒTransformeræ¨¡å‹çš„æ€§èƒ½ï¼Œå‘ç°GPT-4åœ¨BLEUåˆ†æ•°ä¸Šè¡¨ç°æœ€ä½³ã€‚
å®éªŒåœ¨NVIDIA A100 GPUä¸Šè¿›è¡Œï¼Œä½¿ç”¨äº†PyTorchæ¡†æ¶ã€‚
"""

custom_types = ["MODEL_NAME", "METRIC", "DATASET", "HARDWARE", "FRAMEWORK"]
entities = extract_custom_entities(text, custom_types)
```

### åœºæ™¯2ï¼šå…³ç³»æŠ½å–å’ŒéªŒè¯

```python
def extract_and_validate_relations(text: str, entities: list):
    """æŠ½å–å…³ç³»å¹¶è¿›è¡ŒéªŒè¯"""
    
    base_url = "http://localhost:8000/api/v1"
    extract_url = f"{base_url}/knowledge/extract-relations"
    
    payload = {
        "text": text,
        "entities": entities,
        "relation_types": [
            "USES", "ACHIEVES", "COMPARES_WITH", "RUNS_ON", 
            "IMPLEMENTED_IN", "EVALUATED_ON", "OUTPERFORMS"
        ],
        "confidence_threshold": 0.6,
        "validate_relations": True,  # å¯ç”¨å…³ç³»éªŒè¯
        "include_evidence": True     # åŒ…å«è¯æ®æ–‡æœ¬
    }
    
    try:
        response = requests.post(extract_url, json=payload)
        response.raise_for_status()
        
        result = response.json()["data"]
        relations = result["relations"]
        
        print(f"âœ… æŠ½å–åˆ° {len(relations)} ä¸ªå…³ç³»:")
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        relations.sort(key=lambda x: x["confidence"], reverse=True)
        
        for i, relation in enumerate(relations, 1):
            print(f"\n{i}. {relation['source_entity']} --[{relation['relation_type']}]--> {relation['target_entity']}")
            print(f"   ç½®ä¿¡åº¦: {relation['confidence']:.2f}")
            print(f"   è¯æ®: \"{relation['evidence']}\"")
            
            if relation.get('properties'):
                print(f"   å±æ€§: {relation['properties']}")
        
        # éªŒè¯ç»“æœç»Ÿè®¡
        if "validation" in result:
            validation = result["validation"]
            print(f"\nğŸ“Š éªŒè¯ç»Ÿè®¡:")
            print(f"  - é«˜ç½®ä¿¡åº¦å…³ç³»: {validation['high_confidence_count']}")
            print(f"  - ä¸­ç­‰ç½®ä¿¡åº¦å…³ç³»: {validation['medium_confidence_count']}")
            print(f"  - ä½ç½®ä¿¡åº¦å…³ç³»: {validation['low_confidence_count']}")
            print(f"  - å¯èƒ½çš„é”™è¯¯å…³ç³»: {validation['potential_errors']}")
        
        return relations
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ å…³ç³»æŠ½å–å¤±è´¥: {e}")
        return []

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆæ¥ä¸Šé¢çš„å®ä½“æŠ½å–ç»“æœï¼‰
if entities:
    relations = extract_and_validate_relations(text, entities)
```

### åœºæ™¯3ï¼šæ‰¹é‡çŸ¥è¯†æŠ½å–å’Œè¿›åº¦ç›‘æ§

```python
def batch_knowledge_extraction(document_ids: list):
    """æ‰¹é‡çŸ¥è¯†æŠ½å–"""
    
    base_url = "http://localhost:8000/api/v1"
    batch_url = f"{base_url}/knowledge/extract-batch"
    
    payload = {
        "document_ids": document_ids,
        "extraction_options": {
            "extract_entities": True,
            "extract_relations": True,
            "extract_claims": True,
            "entity_types": ["PERSON", "ORGANIZATION", "CONCEPT", "METHOD", "DATASET"],
            "confidence_threshold": 0.8,
            "parallel_processing": True,
            "max_workers": 4
        }
    }
    
    try:
        # æäº¤æ‰¹é‡ä»»åŠ¡
        response = requests.post(batch_url, json=payload)
        response.raise_for_status()
        
        result = response.json()["data"]
        batch_id = result["batch_id"]
        
        print(f"âœ… æ‰¹é‡ä»»åŠ¡å·²æäº¤: {batch_id}")
        print(f"é¢„è®¡å¤„ç†æ—¶é—´: {result['estimated_time']} ç§’")
        
        # ç›‘æ§è¿›åº¦
        status_url = f"{base_url}/knowledge/batch-status/{batch_id}"
        
        while True:
            time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
            
            status_response = requests.get(status_url)
            status_response.raise_for_status()
            
            status_data = status_response.json()["data"]
            
            print(f"è¿›åº¦: {status_data['completed_documents']}/{status_data['total_documents']} "
                  f"({status_data['progress_percentage']:.1f}%)")
            
            if status_data["status"] == "completed":
                print("âœ… æ‰¹é‡æŠ½å–å®Œæˆ!")
                
                # æ˜¾ç¤ºç»“æœç»Ÿè®¡
                stats = status_data["statistics"]
                print(f"ğŸ“Š æŠ½å–ç»Ÿè®¡:")
                print(f"  - æ€»å®ä½“æ•°: {stats['total_entities']}")
                print(f"  - æ€»å…³ç³»æ•°: {stats['total_relations']}")
                print(f"  - æ€»æ–­è¨€æ•°: {stats['total_claims']}")
                print(f"  - å¤„ç†æ—¶é—´: {stats['processing_time']:.2f} ç§’")
                
                break
            elif status_data["status"] == "failed":
                print("âŒ æ‰¹é‡æŠ½å–å¤±è´¥!")
                print(f"é”™è¯¯ä¿¡æ¯: {status_data.get('error_message', 'Unknown error')}")
                break
        
        return batch_id
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ æ‰¹é‡æŠ½å–å¤±è´¥: {e}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
document_ids = ["doc_123", "doc_456", "doc_789"]
batch_id = batch_knowledge_extraction(document_ids)
```

## å›¾æŸ¥è¯¢ç¤ºä¾‹

### åœºæ™¯1ï¼šå®ä½“å…³ç³»æ¢ç´¢

```python
def explore_entity_relationships(entity_name: str, max_depth: int = 2):
    """æ¢ç´¢å®ä½“çš„å…³ç³»ç½‘ç»œ"""
    
    base_url = "http://localhost:8000/api/v1"
    
    # 1. é¦–å…ˆæœç´¢å®ä½“
    search_url = f"{base_url}/graph/nodes/search"
    search_params = {
        "query": entity_name,
        "node_type": "Entity",
        "limit": 1
    }
    
    try:
        response = requests.get(search_url, params=search_params)
        response.raise_for_status()
        
        nodes = response.json()["data"]["nodes"]
        if not nodes:
            print(f"âŒ æœªæ‰¾åˆ°å®ä½“: {entity_name}")
            return
        
        entity_node = nodes[0]
        entity_id = entity_node["id"]
        
        print(f"âœ… æ‰¾åˆ°å®ä½“: {entity_node['properties']['name']}")
        print(f"å®ä½“ID: {entity_id}")
        
        # 2. è·å–å®ä½“çš„ç›´æ¥å…³ç³»
        cypher_url = f"{base_url}/graph/cypher"
        cypher_query = {
            "query": """
            MATCH (e:Entity {id: $entity_id})-[r]-(connected)
            RETURN e, r, connected, type(r) as relation_type
            ORDER BY r.confidence DESC
            LIMIT 20
            """,
            "parameters": {"entity_id": entity_id}
        }
        
        response = requests.post(cypher_url, json=cypher_query)
        response.raise_for_status()
        
        records = response.json()["data"]["records"]
        
        print(f"\nğŸ”— ç›´æ¥å…³ç³» ({len(records)} ä¸ª):")
        
        relation_stats = {}
        for record in records:
            relation_type = record["relation_type"]
            connected_entity = record["connected"]["properties"]["name"]
            confidence = record["r"].get("confidence", 0)
            
            print(f"  - {entity_name} --[{relation_type}]--> {connected_entity} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            # ç»Ÿè®¡å…³ç³»ç±»å‹
            if relation_type not in relation_stats:
                relation_stats[relation_type] = 0
            relation_stats[relation_type] += 1
        
        print(f"\nğŸ“Š å…³ç³»ç±»å‹ç»Ÿè®¡:")
        for rel_type, count in sorted(relation_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"  - {rel_type}: {count}")
        
        # 3. æ¢ç´¢æ›´æ·±å±‚çš„å…³ç³»ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if max_depth > 1:
            print(f"\nğŸŒ æ¢ç´¢ {max_depth} å±‚å…³ç³»ç½‘ç»œ...")
            
            deep_query = {
                "query": f"""
                MATCH path = (e:Entity {{id: $entity_id}})-[*1..{max_depth}]-(connected)
                WHERE connected.id <> $entity_id
                RETURN path, length(path) as depth
                ORDER BY depth, connected.name
                LIMIT 50
                """,
                "parameters": {"entity_id": entity_id}
            }
            
            response = requests.post(cypher_url, json=deep_query)
            response.raise_for_status()
            
            paths = response.json()["data"]["records"]
            
            depth_stats = {}
            for path_record in paths:
                depth = path_record["depth"]
                if depth not in depth_stats:
                    depth_stats[depth] = 0
                depth_stats[depth] += 1
            
            print(f"å‘ç° {len(paths)} æ¡è·¯å¾„:")
            for depth, count in sorted(depth_stats.items()):
                print(f"  - {depth} å±‚: {count} æ¡è·¯å¾„")
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
explore_entity_relationships("äººå·¥æ™ºèƒ½", max_depth=3)
```

### åœºæ™¯2ï¼šç¤¾åŒºå‘ç°å’Œåˆ†æ

```python
def discover_communities(min_community_size: int = 5):
    """å‘ç°å’Œåˆ†æå›¾ä¸­çš„ç¤¾åŒºç»“æ„"""
    
    base_url = "http://localhost:8000/api/v1"
    community_url = f"{base_url}/graph/community-detection"
    
    payload = {
        "algorithm": "louvain",
        "min_community_size": min_community_size,
        "resolution": 1.0,
        "include_statistics": True
    }
    
    try:
        response = requests.post(community_url, json=payload)
        response.raise_for_status()
        
        result = response.json()["data"]
        communities = result["communities"]
        
        print(f"âœ… å‘ç° {len(communities)} ä¸ªç¤¾åŒº:")
        
        # æŒ‰ç¤¾åŒºå¤§å°æ’åº
        communities.sort(key=lambda x: x["size"], reverse=True)
        
        for i, community in enumerate(communities, 1):
            print(f"\nğŸ˜ï¸  ç¤¾åŒº {i} (ID: {community['id']}):")
            print(f"  - èŠ‚ç‚¹æ•°é‡: {community['size']}")
            print(f"  - æ¨¡å—åº¦: {community['modularity']:.3f}")
            print(f"  - ä¸»è¦å®ä½“ç±»å‹: {', '.join(community['dominant_types'])}")
            
            # æ˜¾ç¤ºæ ¸å¿ƒèŠ‚ç‚¹
            if "core_nodes" in community:
                print(f"  - æ ¸å¿ƒèŠ‚ç‚¹:")
                for node in community["core_nodes"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"    â€¢ {node['name']} ({node['type']})")
            
            # æ˜¾ç¤ºä¸»è¦ä¸»é¢˜
            if "topics" in community:
                print(f"  - ä¸»è¦ä¸»é¢˜: {', '.join(community['topics'][:3])}")
        
        # æ˜¾ç¤ºå…¨å±€ç»Ÿè®¡
        if "statistics" in result:
            stats = result["statistics"]
            print(f"\nğŸ“Š ç¤¾åŒºç»Ÿè®¡:")
            print(f"  - æ€»ç¤¾åŒºæ•°: {stats['total_communities']}")
            print(f"  - å¹³å‡ç¤¾åŒºå¤§å°: {stats['average_community_size']:.1f}")
            print(f"  - æœ€å¤§ç¤¾åŒºå¤§å°: {stats['max_community_size']}")
            print(f"  - å…¨å±€æ¨¡å—åº¦: {stats['global_modularity']:.3f}")
            print(f"  - è¦†ç›–ç‡: {stats['coverage']:.1%}")
        
        return communities
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç¤¾åŒºå‘ç°å¤±è´¥: {e}")
        return []

# ä½¿ç”¨ç¤ºä¾‹
communities = discover_communities(min_community_size=3)
```

### åœºæ™¯3ï¼šè·¯å¾„åˆ†æå’Œæ¨ç†

```python
def analyze_entity_paths(source_entity: str, target_entity: str):
    """åˆ†æä¸¤ä¸ªå®ä½“ä¹‹é—´çš„è·¯å¾„"""
    
    base_url = "http://localhost:8000/api/v1"
    
    # 1. æŸ¥æ‰¾å®ä½“ID
    def find_entity_id(entity_name: str):
        search_url = f"{base_url}/graph/nodes/search"
        params = {"query": entity_name, "node_type": "Entity", "limit": 1}
        
        response = requests.get(search_url, params=params)
        response.raise_for_status()
        
        nodes = response.json()["data"]["nodes"]
        return nodes[0]["id"] if nodes else None
    
    try:
        source_id = find_entity_id(source_entity)
        target_id = find_entity_id(target_entity)
        
        if not source_id:
            print(f"âŒ æœªæ‰¾åˆ°æºå®ä½“: {source_entity}")
            return
        if not target_id:
            print(f"âŒ æœªæ‰¾åˆ°ç›®æ ‡å®ä½“: {target_entity}")
            return
        
        print(f"âœ… æ‰¾åˆ°å®ä½“:")
        print(f"  - æºå®ä½“: {source_entity} ({source_id})")
        print(f"  - ç›®æ ‡å®ä½“: {target_entity} ({target_id})")
        
        # 2. æŸ¥æ‰¾è·¯å¾„
        paths_url = f"{base_url}/graph/paths/find"
        paths_payload = {
            "source_node_id": source_id,
            "target_node_id": target_id,
            "max_depth": 4,
            "path_type": "all",  # æŸ¥æ‰¾æ‰€æœ‰è·¯å¾„
            "limit": 10
        }
        
        response = requests.post(paths_url, json=paths_payload)
        response.raise_for_status()
        
        result = response.json()["data"]
        paths = result["paths"]
        
        if not paths:
            print(f"âŒ æœªæ‰¾åˆ° {source_entity} å’Œ {target_entity} ä¹‹é—´çš„è·¯å¾„")
            return
        
        print(f"\nğŸ›¤ï¸  æ‰¾åˆ° {len(paths)} æ¡è·¯å¾„:")
        
        # æŒ‰è·¯å¾„é•¿åº¦å’Œæƒé‡æ’åº
        paths.sort(key=lambda x: (x["length"], -x.get("weight", 0)))
        
        for i, path in enumerate(paths, 1):
            print(f"\nè·¯å¾„ {i} (é•¿åº¦: {path['length']}, æƒé‡: {path.get('weight', 0):.2f}):")
            
            nodes = path["nodes"]
            relationships = path["relationships"]
            
            # æ„å»ºè·¯å¾„å­—ç¬¦ä¸²
            path_str = nodes[0]["name"]
            for j, rel in enumerate(relationships):
                path_str += f" --[{rel['type']}]--> {nodes[j+1]['name']}"
            
            print(f"  {path_str}")
            
            # æ˜¾ç¤ºè·¯å¾„ä¸­çš„å…³é”®ä¿¡æ¯
            if "semantic_similarity" in path:
                print(f"  è¯­ä¹‰ç›¸ä¼¼åº¦: {path['semantic_similarity']:.3f}")
            
            if "confidence_score" in path:
                print(f"  ç½®ä¿¡åº¦åˆ†æ•°: {path['confidence_score']:.3f}")
        
        # 3. è·¯å¾„åˆ†æ
        print(f"\nğŸ“ˆ è·¯å¾„åˆ†æ:")
        
        # æœ€çŸ­è·¯å¾„
        shortest_path = min(paths, key=lambda x: x["length"])
        print(f"  - æœ€çŸ­è·¯å¾„é•¿åº¦: {shortest_path['length']}")
        
        # æœ€é«˜æƒé‡è·¯å¾„
        if any("weight" in p for p in paths):
            highest_weight_path = max(paths, key=lambda x: x.get("weight", 0))
            print(f"  - æœ€é«˜æƒé‡è·¯å¾„: {highest_weight_path.get('weight', 0):.2f}")
        
        # å…³ç³»ç±»å‹ç»Ÿè®¡
        relation_types = []
        for path in paths:
            for rel in path["relationships"]:
                relation_types.append(rel["type"])
        
        from collections import Counter
        relation_counts = Counter(relation_types)
        
        print(f"  - å¸¸è§å…³ç³»ç±»å‹:")
        for rel_type, count in relation_counts.most_common(3):
            print(f"    â€¢ {rel_type}: {count} æ¬¡")
        
        return paths
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è·¯å¾„åˆ†æå¤±è´¥: {e}")
        return []

# ä½¿ç”¨ç¤ºä¾‹
paths = analyze_entity_paths("äººå·¥æ™ºèƒ½", "æ·±åº¦å­¦ä¹ ")
```

## RAG é—®ç­”ç¤ºä¾‹

### åœºæ™¯1ï¼šå¤šè½®å¯¹è¯é—®ç­”

```python
class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8000/api/v1"):
        self.base_url = base_url
        self.conversation_id = None
        self.conversation_history = []
    
    def start_conversation(self):
        """å¼€å§‹æ–°å¯¹è¯"""
        import uuid
        self.conversation_id = str(uuid.uuid4())
        self.conversation_history = []
        print(f"âœ… å¼€å§‹æ–°å¯¹è¯: {self.conversation_id}")
    
    def ask(self, question: str, include_sources: bool = True):
        """æé—®"""
        if not self.conversation_id:
            self.start_conversation()
        
        conversation_url = f"{self.base_url}/rag/conversation"
        
        payload = {
            "message": question,
            "conversation_id": self.conversation_id,
            "context_window": 5,
            "maintain_context": True,
            "include_sources": include_sources,
            "response_format": "detailed"
        }
        
        try:
            response = requests.post(conversation_url, json=payload)
            response.raise_for_status()
            
            result = response.json()["data"]
            answer = result["response"]
            sources = result.get("sources", [])
            
            # æ·»åŠ åˆ°å¯¹è¯å†å²
            self.conversation_history.append({
                "role": "user",
                "content": question,
                "timestamp": time.time()
            })
            self.conversation_history.append({
                "role": "assistant",
                "content": answer,
                "sources": sources,
                "timestamp": time.time()
            })
            
            print(f"\nğŸ¤– åŠ©æ‰‹: {answer}")
            
            if sources and include_sources:
                print(f"\nğŸ“š å‚è€ƒæ¥æº:")
                for i, source in enumerate(sources[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªæ¥æº
                    print(f"  {i}. {source['document_title']} (ç›¸å…³åº¦: {source['relevance_score']:.2f})")
                    if source.get('page'):
                        print(f"     é¡µç : {source['page']}")
            
            return answer, sources
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ é—®ç­”å¤±è´¥: {e}")
            return None, []
    
    def get_conversation_summary(self):
        """è·å–å¯¹è¯æ‘˜è¦"""
        if not self.conversation_history:
            print("ğŸ“ å¯¹è¯å†å²ä¸ºç©º")
            return
        
        print(f"\nğŸ“ å¯¹è¯æ‘˜è¦ (å¯¹è¯ID: {self.conversation_id}):")
        print(f"æ€»è½®æ¬¡: {len([msg for msg in self.conversation_history if msg['role'] == 'user'])}")
        
        for i, message in enumerate(self.conversation_history):
            if message["role"] == "user":
                print(f"\nğŸ‘¤ ç”¨æˆ·: {message['content']}")
            else:
                print(f"ğŸ¤– åŠ©æ‰‹: {message['content'][:100]}...")
                if message.get('sources'):
                    print(f"   ğŸ“š å¼•ç”¨äº† {len(message['sources'])} ä¸ªæ¥æº")

# ä½¿ç”¨ç¤ºä¾‹
def demo_conversation():
    """æ¼”ç¤ºå¤šè½®å¯¹è¯"""
    
    chat = ConversationManager()
    
    # ç¬¬ä¸€è½®ï¼šåŸºç¡€é—®é¢˜
    print("=" * 50)
    print("ç¬¬ä¸€è½®ï¼šåŸºç¡€é—®é¢˜")
    print("=" * 50)
    chat.ask("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
    
    # ç¬¬äºŒè½®ï¼šæ·±å…¥é—®é¢˜
    print("\n" + "=" * 50)
    print("ç¬¬äºŒè½®ï¼šæ·±å…¥é—®é¢˜")
    print("=" * 50)
    chat.ask("äººå·¥æ™ºèƒ½æœ‰å“ªäº›ä¸»è¦çš„åº”ç”¨é¢†åŸŸï¼Ÿ")
    
    # ç¬¬ä¸‰è½®ï¼šå…³è”é—®é¢˜
    print("\n" + "=" * 50)
    print("ç¬¬ä¸‰è½®ï¼šå…³è”é—®é¢˜")
    print("=" * 50)
    chat.ask("æ·±åº¦å­¦ä¹ åœ¨è¿™äº›åº”ç”¨ä¸­èµ·åˆ°ä»€ä¹ˆä½œç”¨ï¼Ÿ")
    
    # ç¬¬å››è½®ï¼šå…·ä½“é—®é¢˜
    print("\n" + "=" * 50)
    print("ç¬¬å››è½®ï¼šå…·ä½“é—®é¢˜")
    print("=" * 50)
    chat.ask("èƒ½ç»™æˆ‘ä¸€äº›æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„å…·ä½“ä¾‹å­å—ï¼Ÿ")
    
    # æ˜¾ç¤ºå¯¹è¯æ‘˜è¦
    chat.get_conversation_summary()

# è¿è¡Œæ¼”ç¤º
demo_conversation()
```

### åœºæ™¯2ï¼šå¤šæ¨¡æ€æŸ¥è¯¢

```python
def multimodal_query_example():
    """å¤šæ¨¡æ€æŸ¥è¯¢ç¤ºä¾‹"""
    
    base_url = "http://localhost:8000/api/v1"
    multimodal_url = f"{base_url}/rag/multimodal-query"
    
    # å‡†å¤‡å›¾åƒå’Œæ–‡æœ¬æŸ¥è¯¢
    image_path = "diagram.png"  # å‡è®¾æœ‰ä¸€ä¸ªæŠ€æœ¯å›¾è¡¨
    text_query = "è¿™ä¸ªå›¾è¡¨å±•ç¤ºçš„æ˜¯ä»€ä¹ˆæŠ€æœ¯æ¶æ„ï¼Ÿè¯·è¯¦ç»†è§£é‡Šå„ä¸ªç»„ä»¶çš„ä½œç”¨ã€‚"
    
    try:
        # å‡†å¤‡æ–‡ä»¶å’Œæ•°æ®
        files = {
            'image': ('diagram.png', open(image_path, 'rb'), 'image/png')
        }
        
        data = {
            'query': text_query,
            'query_options': json.dumps({
                'include_sources': True,
                'analysis_depth': 'detailed',
                'extract_text_from_image': True,
                'identify_objects': True,
                'describe_relationships': True
            })
        }
        
        response = requests.post(multimodal_url, files=files, data=data)
        response.raise_for_status()
        
        result = response.json()["data"]
        
        print("ğŸ–¼ï¸  å¤šæ¨¡æ€æŸ¥è¯¢ç»“æœ:")
        print(f"ğŸ“ æ–‡æœ¬æŸ¥è¯¢: {text_query}")
        print(f"ğŸ¤– AI å›ç­”: {result['response']}")
        
        # å›¾åƒåˆ†æç»“æœ
        if "image_analysis" in result:
            analysis = result["image_analysis"]
            print(f"\nğŸ” å›¾åƒåˆ†æ:")
            
            if "detected_objects" in analysis:
                print(f"  - æ£€æµ‹åˆ°çš„å¯¹è±¡: {', '.join(analysis['detected_objects'])}")
            
            if "extracted_text" in analysis:
                print(f"  - æå–çš„æ–‡æœ¬: {analysis['extracted_text']}")
            
            if "scene_description" in analysis:
                print(f"  - åœºæ™¯æè¿°: {analysis['scene_description']}")
        
        # ç›¸å…³æ–‡æ¡£
        if "sources" in result:
            print(f"\nğŸ“š ç›¸å…³æ–‡æ¡£:")
            for source in result["sources"][:3]:
                print(f"  - {source['document_title']} (ç›¸å…³åº¦: {source['relevance_score']:.2f})")
        
    except FileNotFoundError:
        print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
    except requests.exceptions.RequestException as e:
        print(f"âŒ å¤šæ¨¡æ€æŸ¥è¯¢å¤±è´¥: {e}")
    finally:
        if 'files' in locals():
            files['image'][1].close()

# ä½¿ç”¨ç¤ºä¾‹
multimodal_query_example()
```

### åœºæ™¯3ï¼šæ‰¹é‡é—®ç­”å’Œç»“æœåˆ†æ

```python
def batch_qa_analysis(questions: list):
    """æ‰¹é‡é—®ç­”å’Œç»“æœåˆ†æ"""
    
    base_url = "http://localhost:8000/api/v1"
    batch_url = f"{base_url}/rag/batch-query"
    
    payload = {
        "queries": [
            {
                "id": f"q_{i}",
                "query": question,
                "query_type": "qa",
                "include_sources": True
            }
            for i, question in enumerate(questions, 1)
        ],
        "processing_options": {
            "parallel_processing": True,
            "max_workers": 3,
            "timeout_per_query": 30
        }
    }
    
    try:
        # æäº¤æ‰¹é‡æŸ¥è¯¢
        response = requests.post(batch_url, json=payload)
        response.raise_for_status()
        
        result = response.json()["data"]
        batch_id = result["batch_id"]
        
        print(f"âœ… æ‰¹é‡æŸ¥è¯¢å·²æäº¤: {batch_id}")
        print(f"æŸ¥è¯¢æ•°é‡: {len(questions)}")
        
        # ç­‰å¾…ç»“æœ
        status_url = f"{base_url}/rag/batch-status/{batch_id}"
        
        while True:
            time.sleep(5)
            
            status_response = requests.get(status_url)
            status_response.raise_for_status()
            
            status_data = status_response.json()["data"]
            
            if status_data["status"] == "completed":
                break
            elif status_data["status"] == "failed":
                print("âŒ æ‰¹é‡æŸ¥è¯¢å¤±è´¥")
                return
            
            print(f"è¿›åº¦: {status_data['completed_queries']}/{status_data['total_queries']}")
        
        # è·å–ç»“æœ
        results_url = f"{base_url}/rag/batch-results/{batch_id}"
        results_response = requests.get(results_url)
        results_response.raise_for_status()
        
        results = results_response.json()["data"]["results"]
        
        print(f"\nâœ… æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼Œå…± {len(results)} ä¸ªç»“æœ:")
        
        # åˆ†æç»“æœ
        total_confidence = 0
        total_sources = 0
        response_lengths = []
        
        for i, result in enumerate(results, 1):
            query_result = result["result"]
            
            print(f"\nğŸ“ é—®é¢˜ {i}: {result['query']}")
            print(f"ğŸ¤– å›ç­”: {query_result['answer'][:200]}...")
            print(f"ğŸ“Š ç½®ä¿¡åº¦: {query_result['confidence']:.2f}")
            print(f"ğŸ“š æ¥æºæ•°é‡: {len(query_result.get('sources', []))}")
            
            # ç»Ÿè®¡æ•°æ®
            total_confidence += query_result['confidence']
            total_sources += len(query_result.get('sources', []))
            response_lengths.append(len(query_result['answer']))
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æ‰¹é‡æŸ¥è¯¢ç»Ÿè®¡:")
        print(f"  - å¹³å‡ç½®ä¿¡åº¦: {total_confidence / len(results):.2f}")
        print(f"  - å¹³å‡æ¥æºæ•°é‡: {total_sources / len(results):.1f}")
        print(f"  - å¹³å‡å›ç­”é•¿åº¦: {sum(response_lengths) / len(response_lengths):.0f} å­—ç¬¦")
        print(f"  - æœ€é•¿å›ç­”: {max(response_lengths)} å­—ç¬¦")
        print(f"  - æœ€çŸ­å›ç­”: {min(response_lengths)} å­—ç¬¦")
        
        return results
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ æ‰¹é‡æŸ¥è¯¢å¤±è´¥: {e}")
        return []

# ä½¿ç”¨ç¤ºä¾‹
questions = [
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
    "è‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸»è¦ä»»åŠ¡æœ‰å“ªäº›ï¼Ÿ",
    "è®¡ç®—æœºè§†è§‰åœ¨å®é™…åº”ç”¨ä¸­æœ‰å“ªäº›æŒ‘æˆ˜ï¼Ÿ",
    "å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"
]

results = batch_qa_analysis(questions)
```

## å®Œæ•´å·¥ä½œæµç¨‹

### åœºæ™¯ï¼šæ„å»ºå­¦æœ¯è®ºæ–‡çŸ¥è¯†åº“

```python
import os
import time
import requests
from pathlib import Path

class AcademicKnowledgeBase:
    """å­¦æœ¯è®ºæ–‡çŸ¥è¯†åº“æ„å»ºå™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8000/api/v1"):
        self.base_url = base_url
        self.session = requests.Session()
        
    def build_knowledge_base(self, papers_directory: str):
        """æ„å»ºå®Œæ•´çš„å­¦æœ¯è®ºæ–‡çŸ¥è¯†åº“"""
        
        print("ğŸš€ å¼€å§‹æ„å»ºå­¦æœ¯è®ºæ–‡çŸ¥è¯†åº“")
        print("=" * 50)
        
        # æ­¥éª¤1ï¼šä¸Šä¼ è®ºæ–‡
        print("\nğŸ“¤ æ­¥éª¤1ï¼šä¸Šä¼ è®ºæ–‡")
        document_ids = self._upload_papers(papers_directory)
        
        if not document_ids:
            print("âŒ æ²¡æœ‰æˆåŠŸä¸Šä¼ çš„è®ºæ–‡")
            return
        
        # æ­¥éª¤2ï¼šå¤„ç†æ–‡æ¡£
        print("\nâš™ï¸  æ­¥éª¤2ï¼šå¤„ç†æ–‡æ¡£")
        self._process_documents(document_ids)
        
        # æ­¥éª¤3ï¼šçŸ¥è¯†æŠ½å–
        print("\nğŸ§  æ­¥éª¤3ï¼šçŸ¥è¯†æŠ½å–")
        self._extract_knowledge(document_ids)
        
        # æ­¥éª¤4ï¼šæ„å»ºçŸ¥è¯†å›¾è°±
        print("\nğŸ•¸ï¸  æ­¥éª¤4ï¼šæ„å»ºçŸ¥è¯†å›¾è°±")
        self._build_knowledge_graph()
        
        # æ­¥éª¤5ï¼šéªŒè¯å’Œæµ‹è¯•
        print("\nâœ… æ­¥éª¤5ï¼šéªŒè¯å’Œæµ‹è¯•")
        self._validate_knowledge_base()
        
        print("\nğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
        
    def _upload_papers(self, papers_directory: str):
        """ä¸Šä¼ è®ºæ–‡"""
        
        papers_path = Path(papers_directory)
        pdf_files = list(papers_path.glob("*.pdf"))
        
        print(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        document_ids = []
        batch_size = 5
        
        for i in range(0, len(pdf_files), batch_size):
            batch_files = pdf_files[i:i+batch_size]
            
            files = []
            for pdf_file in batch_files:
                files.append(('files', (pdf_file.name, open(pdf_file, 'rb'), 'application/pdf')))
            
            metadata = {
                "category": "academic_paper",
                "tags": ["research", "academic"],
                "batch_id": f"batch_{i//batch_size + 1}"
            }
            
            data = {"metadata": json.dumps(metadata)}
            
            try:
                upload_url = f"{self.base_url}/document-management/upload"
                response = self.session.post(upload_url, files=files, data=data)
                response.raise_for_status()
                
                result = response.json()["data"]
                batch_doc_ids = [file_info["id"] for file_info in result["uploaded_files"]]
                document_ids.extend(batch_doc_ids)
                
                print(f"  âœ… æ‰¹æ¬¡ {i//batch_size + 1}: ä¸Šä¼  {len(batch_doc_ids)} ä¸ªæ–‡ä»¶")
                
            except Exception as e:
                print(f"  âŒ æ‰¹æ¬¡ {i//batch_size + 1} ä¸Šä¼ å¤±è´¥: {e}")
            
            finally:
                for _, (_, file_obj, _) in files:
                    file_obj.close()
        
        print(f"ğŸ“Š æ€»è®¡ä¸Šä¼  {len(document_ids)} ä¸ªæ–‡æ¡£")
        return document_ids
    
    def _process_documents(self, document_ids: list):
        """å¤„ç†æ–‡æ¡£"""
        
        processing_options = {
            "extract_text": True,
            "create_chunks": True,
            "generate_embeddings": True,
            "extract_entities": False,  # ç¨åæ‰¹é‡å¤„ç†
            "chunk_size": 1000,
            "chunk_overlap": 200
        }
        
        processed_count = 0
        
        for doc_id in document_ids:
            try:
                process_url = f"{self.base_url}/document-management/process/{doc_id}"
                payload = {"processing_options": processing_options}
                
                response = self.session.post(process_url, json=payload)
                response.raise_for_status()
                
                # ç­‰å¾…å¤„ç†å®Œæˆ
                self._wait_for_processing(doc_id)
                processed_count += 1
                
                print(f"  âœ… å¤„ç†å®Œæˆ: {doc_id} ({processed_count}/{len(document_ids)})")
                
            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {doc_id} - {e}")
        
        print(f"ğŸ“Š æˆåŠŸå¤„ç† {processed_count} ä¸ªæ–‡æ¡£")
    
    def _wait_for_processing(self, document_id: str, timeout: int = 300):
        """ç­‰å¾…æ–‡æ¡£å¤„ç†å®Œæˆ"""
        
        start_time = time.time()
        status_url = f"{self.base_url}/document-management/documents/{document_id}"
        
        while time.time() - start_time < timeout:
            try:
                response = self.session.get(status_url)
                response.raise_for_status()
                
                doc_info = response.json()["data"]
                status = doc_info["status"]
                
                if status == "processed":
                    return True
                elif status == "failed":
                    raise Exception(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {doc_info.get('error_message', 'Unknown error')}")
                
                time.sleep(5)
                
            except Exception as e:
                raise Exception(f"æ£€æŸ¥å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
        
        raise Exception("æ–‡æ¡£å¤„ç†è¶…æ—¶")
    
    def _extract_knowledge(self, document_ids: list):
        """æ‰¹é‡çŸ¥è¯†æŠ½å–"""
        
        batch_url = f"{self.base_url}/knowledge/extract-batch"
        
        payload = {
            "document_ids": document_ids,
            "extraction_options": {
                "extract_entities": True,
                "extract_relations": True,
                "extract_claims": True,
                "entity_types": [
                    "PERSON", "ORGANIZATION", "CONCEPT", "METHOD", 
                    "DATASET", "METRIC", "TECHNOLOGY", "LOCATION"
                ],
                "confidence_threshold": 0.7,
                "parallel_processing": True,
                "max_workers": 4
            }
        }
        
        try:
            response = self.session.post(batch_url, json=payload)
            response.raise_for_status()
            
            result = response.json()["data"]
            batch_id = result["batch_id"]
            
            print(f"  ğŸ“‹ æ‰¹é‡æŠ½å–ä»»åŠ¡ID: {batch_id}")
            
            # ç›‘æ§è¿›åº¦
            self._monitor_batch_extraction(batch_id)
            
        except Exception as e:
            print(f"  âŒ æ‰¹é‡çŸ¥è¯†æŠ½å–å¤±è´¥: {e}")
    
    def _monitor_batch_extraction(self, batch_id: str):
        """ç›‘æ§æ‰¹é‡æŠ½å–è¿›åº¦"""
        
        status_url = f"{self.base_url}/knowledge/batch-status/{batch_id}"
        
        while True:
            try:
                response = self.session.get(status_url)
                response.raise_for_status()
                
                status_data = response.json()["data"]
                
                progress = status_data["progress_percentage"]
                print(f"  ğŸ“Š æŠ½å–è¿›åº¦: {progress:.1f}%")
                
                if status_data["status"] == "completed":
                    stats = status_data["statistics"]
                    print(f"  âœ… æŠ½å–å®Œæˆ:")
                    print(f"    - å®ä½“: {stats['total_entities']}")
                    print(f"    - å…³ç³»: {stats['total_relations']}")
                    print(f"    - æ–­è¨€: {stats['total_claims']}")
                    break
                elif status_data["status"] == "failed":
                    print(f"  âŒ æŠ½å–å¤±è´¥: {status_data.get('error_message', 'Unknown error')}")
                    break
                
                time.sleep(10)
                
            except Exception as e:
                print(f"  âŒ ç›‘æ§æŠ½å–è¿›åº¦å¤±è´¥: {e}")
                break
    
    def _build_knowledge_graph(self):
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        
        # ç¤¾åŒºå‘ç°
        print("  ğŸ˜ï¸  å‘ç°ç¤¾åŒºç»“æ„...")
        community_url = f"{self.base_url}/graph/community-detection"
        
        try:
            payload = {
                "algorithm": "louvain",
                "min_community_size": 3,
                "resolution": 1.0
            }
            
            response = self.session.post(community_url, json=payload)
            response.raise_for_status()
            
            communities = response.json()["data"]["communities"]
            print(f"    å‘ç° {len(communities)} ä¸ªç¤¾åŒº")
            
        except Exception as e:
            print(f"    âŒ ç¤¾åŒºå‘ç°å¤±è´¥: {e}")
        
        # ä¸­å¿ƒæ€§åˆ†æ
        print("  ğŸ“Š è®¡ç®—èŠ‚ç‚¹ä¸­å¿ƒæ€§...")
        centrality_url = f"{self.base_url}/graph/centrality-analysis"
        
        try:
            payload = {
                "algorithms": ["betweenness", "closeness", "pagerank"],
                "node_types": ["Entity"]
            }
            
            response = self.session.post(centrality_url, json=payload)
            response.raise_for_status()
            
            print("    âœ… ä¸­å¿ƒæ€§åˆ†æå®Œæˆ")
            
        except Exception as e:
            print(f"    âŒ ä¸­å¿ƒæ€§åˆ†æå¤±è´¥: {e}")
    
    def _validate_knowledge_base(self):
        """éªŒè¯çŸ¥è¯†åº“"""
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "çŸ¥è¯†åº“ä¸­æœ‰å¤šå°‘ç¯‡è®ºæ–‡ï¼Ÿ",
            "ä¸»è¦çš„ç ”ç©¶ä¸»é¢˜æœ‰å“ªäº›ï¼Ÿ",
            "æœ€é‡è¦çš„ç ”ç©¶è€…æ˜¯è°ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ ç›¸å…³çš„è®ºæ–‡æœ‰å“ªäº›ï¼Ÿ"
        ]
        
        print("  ğŸ§ª æµ‹è¯•æŸ¥è¯¢:")
        
        for query in test_queries:
            try:
                query_url = f"{self.base_url}/rag/query"
                payload = {
                    "query": query,
                    "include_sources": True
                }
                
                response = self.session.post(query_url, json=payload)
                response.raise_for_status()
                
                result = response.json()["data"]
                answer = result["answer"]
                confidence = result["confidence"]
                
                print(f"    â“ {query}")
                print(f"    ğŸ¤– {answer[:100]}... (ç½®ä¿¡åº¦: {confidence:.2f})")
                
            except Exception as e:
                print(f"    âŒ æŸ¥è¯¢å¤±è´¥: {query} - {e}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        print("\n  ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡:")
        try:
            stats_url = f"{self.base_url}/system/statistics"
            response = self.session.get(stats_url)
            response.raise_for_status()
            
            stats = response.json()["data"]
            print(f"    - æ–‡æ¡£æ•°é‡: {stats.get('documents', 0)}")
            print(f"    - å®ä½“æ•°é‡: {stats.get('entities', 0)}")
            print(f"    - å…³ç³»æ•°é‡: {stats.get('relationships', 0)}")
            print(f"    - æ–‡æœ¬å—æ•°é‡: {stats.get('chunks', 0)}")
            
        except Exception as e:
            print(f"    âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
def build_academic_kb():
    """æ„å»ºå­¦æœ¯çŸ¥è¯†åº“ç¤ºä¾‹"""
    
    kb_builder = AcademicKnowledgeBase()
    
    # æŒ‡å®šè®ºæ–‡ç›®å½•
    papers_directory = "./academic_papers"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists(papers_directory):
        print(f"âŒ è®ºæ–‡ç›®å½•ä¸å­˜åœ¨: {papers_directory}")
        return
    
    # æ„å»ºçŸ¥è¯†åº“
    kb_builder.build_knowledge_base(papers_directory)

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    build_academic_kb()
```

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰å¤„ç†æµæ°´çº¿

```python
class CustomProcessingPipeline:
    """è‡ªå®šä¹‰å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, base_url: str = "http://localhost:8000/api/v1"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def create_custom_pipeline(self, pipeline_config: dict):
        """åˆ›å»ºè‡ªå®šä¹‰å¤„ç†æµæ°´çº¿"""
        
        pipeline_url = f"{self.base_url}/processing/create-pipeline"
        
        try:
            response = self.session.post(pipeline_url, json=pipeline_config)
            response.raise_for_status()
            
            result = response.json()["data"]
            pipeline_id = result["pipeline_id"]
            
            print(f"âœ… åˆ›å»ºè‡ªå®šä¹‰æµæ°´çº¿: {pipeline_id}")
            return pipeline_id
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºæµæ°´çº¿å¤±è´¥: {e}")
            return None

# ä½¿ç”¨ç¤ºä¾‹
pipeline_config = {
    "name": "Academic Paper Processing",
    "description": "ä¸“é—¨å¤„ç†å­¦æœ¯è®ºæ–‡çš„æµæ°´çº¿",
    "steps": [
        {
            "name": "text_extraction",
            "type": "text_extractor",
            "config": {
                "extract_tables": True,
                "extract_figures": True,
                "preserve_formatting": True
            }
        },
        {
            "name": "academic_chunking",
            "type": "chunker",
            "config": {
                "strategy": "semantic_sections",
                "respect_sections": True,
                "min_chunk_size": 500,
                "max_chunk_size": 1500
            }
        },
        {
            "name": "academic_entity_extraction",
            "type": "entity_extractor",
            "config": {
                "models": ["academic_ner", "general_ner"],
                "entity_types": [
                    "AUTHOR", "INSTITUTION", "PUBLICATION", 
                    "METHOD", "DATASET", "METRIC", "CONCEPT"
                ],
                "confidence_threshold": 0.8
            }
        }
    ]
}

pipeline = CustomProcessingPipeline()
pipeline_id = pipeline.create_custom_pipeline(pipeline_config)
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

```python
def diagnose_system_issues():
    """è¯Šæ–­ç³»ç»Ÿé—®é¢˜"""
    
    base_url = "http://localhost:8000/api/v1"
    
    print("ğŸ” ç³»ç»Ÿè¯Šæ–­å¼€å§‹...")
    
    # 1. æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€
    print("\n1. æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€")
    try:
        health_response = requests.get(f"{base_url}/system/health", timeout=10)
        health_response.raise_for_status()
        
        health_data = health_response.json()
        print(f"  âœ… ç³»ç»ŸçŠ¶æ€: {health_data['status']}")
        
        services = health_data.get('services', {})
        for service, status in services.items():
            status_icon = "âœ…" if status == "healthy" else "âŒ"
            print(f"  {status_icon} {service}: {status}")
            
    except requests.exceptions.Timeout:
        print("  âŒ ç³»ç»Ÿå“åº”è¶…æ—¶")
    except requests.exceptions.ConnectionError:
        print("  âŒ æ— æ³•è¿æ¥åˆ°ç³»ç»Ÿ")
    except Exception as e:
        print(f"  âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
    
    # 2. æ£€æŸ¥æ•°æ®åº“è¿æ¥
    print("\n2. æ£€æŸ¥æ•°æ®åº“è¿æ¥")
    try:
        db_response = requests.get(f"{base_url}/system/database-status", timeout=5)
        db_response.raise_for_status()
        
        db_data = db_response.json()["data"]
        
        for db_name, db_info in db_data.items():
            status_icon = "âœ…" if db_info["status"] == "connected" else "âŒ"
            print(f"  {status_icon} {db_name}: {db_info['status']}")
            
            if db_info["status"] != "connected":
                print(f"    é”™è¯¯: {db_info.get('error', 'Unknown error')}")
                
    except Exception as e:
        print(f"  âŒ æ•°æ®åº“çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
    
    # 3. æ£€æŸ¥å­˜å‚¨ç©ºé—´
    print("\n3. æ£€æŸ¥å­˜å‚¨ç©ºé—´")
    try:
        storage_response = requests.get(f"{base_url}/system/storage-info", timeout=5)
        storage_response.raise_for_status()
        
        storage_data = storage_response.json()["data"]
        
        for storage_type, info in storage_data.items():
            used_percent = (info["used"] / info["total"]) * 100
            status_icon = "âš ï¸" if used_percent > 80 else "âœ…"
            
            print(f"  {status_icon} {storage_type}: {used_percent:.1f}% å·²ä½¿ç”¨")
            print(f"    æ€»ç©ºé—´: {info['total'] / (1024**3):.1f} GB")
            print(f"    å·²ä½¿ç”¨: {info['used'] / (1024**3):.1f} GB")
            
    except Exception as e:
        print(f"  âŒ å­˜å‚¨ä¿¡æ¯æ£€æŸ¥å¤±è´¥: {e}")
    
    # 4. æ£€æŸ¥æœ€è¿‘çš„é”™è¯¯æ—¥å¿—
    print("\n4. æ£€æŸ¥æœ€è¿‘çš„é”™è¯¯")
    try:
        logs_response = requests.get(f"{base_url}/system/recent-errors", timeout=5)
        logs_response.raise_for_status()
        
        errors = logs_response.json()["data"]["errors"]
        
        if errors:
            print(f"  âš ï¸  å‘ç° {len(errors)} ä¸ªæœ€è¿‘çš„é”™è¯¯:")
            for error in errors[-5:]:  # åªæ˜¾ç¤ºæœ€è¿‘5ä¸ªé”™è¯¯
                print(f"    - {error['timestamp']}: {error['message']}")
        else:
            print("  âœ… æ²¡æœ‰å‘ç°æœ€è¿‘çš„é”™è¯¯")
            
    except Exception as e:
        print(f"  âŒ é”™è¯¯æ—¥å¿—æ£€æŸ¥å¤±è´¥: {e}")
    
    print("\nğŸ” ç³»ç»Ÿè¯Šæ–­å®Œæˆ")

# è¿è¡Œè¯Šæ–­
diagnose_system_issues()
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

```python
def performance_optimization_tips():
    """æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    
    print("ğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    print("=" * 40)
    
    print("\nğŸ“¤ æ–‡æ¡£ä¸Šä¼ ä¼˜åŒ–:")
    print("  - ä½¿ç”¨æ‰¹é‡ä¸Šä¼ ï¼Œæ¯æ‰¹5-10ä¸ªæ–‡ä»¶")
    print("  - å‹ç¼©å¤§æ–‡ä»¶åå†ä¸Šä¼ ")
    print("  - é¿å…åœ¨é«˜å³°æœŸä¸Šä¼ å¤§é‡æ–‡ä»¶")
    
    print("\nğŸ§  çŸ¥è¯†æŠ½å–ä¼˜åŒ–:")
    print("  - æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©åˆé€‚çš„æŠ½å–ç­–ç•¥")
    print("  - è®¾ç½®åˆç†çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆ0.7-0.8ï¼‰")
    print("  - ä½¿ç”¨å¹¶è¡Œå¤„ç†åŠ é€Ÿæ‰¹é‡æŠ½å–")
    
    print("\nğŸ•¸ï¸  å›¾æŸ¥è¯¢ä¼˜åŒ–:")
    print("  - ä½¿ç”¨ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½")
    print("  - é™åˆ¶æŸ¥è¯¢æ·±åº¦ï¼ˆå»ºè®®ä¸è¶…è¿‡4å±‚ï¼‰")
    print("  - ç¼“å­˜å¸¸ç”¨æŸ¥è¯¢ç»“æœ")
    
    print("\nğŸ’¬ RAG é—®ç­”ä¼˜åŒ–:")
    print("  - ä½¿ç”¨å¯¹è¯ä¸Šä¸‹æ–‡æé«˜å›ç­”è´¨é‡")
    print("  - è®¾ç½®åˆé€‚çš„æ£€ç´¢æ•°é‡ï¼ˆ5-10ä¸ªç›¸å…³æ–‡æ¡£ï¼‰")
    print("  - æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©æ£€ç´¢ç­–ç•¥")
    
    print("\nğŸ”§ ç³»ç»Ÿé…ç½®ä¼˜åŒ–:")
    print("  - æ ¹æ®ç¡¬ä»¶é…ç½®è°ƒæ•´å¹¶å‘æ•°")
    print("  - å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜")
    print("  - ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ")

# è¿è¡Œä¼˜åŒ–å»ºè®®
performance_optimization_tips()
```

## æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº† GraphRAG çŸ¥è¯†åº“ç³»ç»Ÿçš„è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹ï¼Œæ¶µç›–äº†ä»åŸºç¡€æ“ä½œåˆ°é«˜çº§ç”¨æ³•çš„å„ç§åœºæ™¯ã€‚é€šè¿‡è¿™äº›ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥ï¼š

1. **å¿«é€Ÿä¸Šæ‰‹**ï¼šé€šè¿‡ç®€å•çš„ç¤ºä¾‹äº†è§£ç³»ç»Ÿçš„åŸºæœ¬åŠŸèƒ½
2. **æ·±å…¥å­¦ä¹ **ï¼šé€šè¿‡å¤æ‚çš„å·¥ä½œæµç¨‹æŒæ¡ç³»ç»Ÿçš„é«˜çº§ç‰¹æ€§
3. **å®é™…åº”ç”¨**ï¼šå‚è€ƒå®Œæ•´çš„é¡¹ç›®ç¤ºä¾‹æ„å»ºè‡ªå·±çš„çŸ¥è¯†åº“
4. **é—®é¢˜è§£å†³**ï¼šä½¿ç”¨æ•…éšœæ’é™¤æŒ‡å—è§£å†³å¸¸è§é—®é¢˜
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ ¹æ®ä¼˜åŒ–å»ºè®®æå‡ç³»ç»Ÿæ€§èƒ½

### æœ€ä½³å®è·µæ€»ç»“

1. **æ–‡æ¡£ç®¡ç†**ï¼š
   - æ‰¹é‡ä¸Šä¼ æ–‡ä»¶ä»¥æé«˜æ•ˆç‡
   - ä¸ºæ–‡æ¡£æ·»åŠ è¯¦ç»†çš„å…ƒæ•°æ®
   - å®šæœŸç›‘æ§æ–‡æ¡£å¤„ç†çŠ¶æ€

2. **çŸ¥è¯†æŠ½å–**ï¼š
   - æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©åˆé€‚çš„æŠ½å–ç­–ç•¥
   - è®¾ç½®åˆç†çš„ç½®ä¿¡åº¦é˜ˆå€¼
   - ä½¿ç”¨å¹¶è¡Œå¤„ç†åŠ é€Ÿæ‰¹é‡æ“ä½œ

3. **å›¾æŸ¥è¯¢**ï¼š
   - åˆç†è®¾è®¡æŸ¥è¯¢æ·±åº¦
   - åˆ©ç”¨ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
   - ç¼“å­˜å¸¸ç”¨æŸ¥è¯¢ç»“æœ

4. **RAG é—®ç­”**ï¼š
   - ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡ä»¥æé«˜å›ç­”è´¨é‡
   - æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©æ£€ç´¢ç­–ç•¥
   - åŒ…å«æ¥æºä¿¡æ¯ä»¥æé«˜å¯ä¿¡åº¦

5. **ç³»ç»Ÿç»´æŠ¤**ï¼š
   - å®šæœŸè¿›è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥
   - ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ
   - åŠæ—¶å¤„ç†é”™è¯¯å’Œå¼‚å¸¸

### ä¸‹ä¸€æ­¥

- æŸ¥çœ‹ [API æ–‡æ¡£](API_DOCUMENTATION.md) äº†è§£è¯¦ç»†çš„æ¥å£è¯´æ˜
- å‚è€ƒ [éƒ¨ç½²æŒ‡å—](DEPLOYMENT_GUIDE.md) è¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- é˜…è¯» [å¼€å‘æŒ‡å—](DEVELOPMENT_GUIDE.md) äº†è§£ç³»ç»Ÿæ¶æ„å’Œæ‰©å±•æ–¹æ³•

å¦‚æœ‰é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·å‚è€ƒæ•…éšœæ’é™¤éƒ¨åˆ†æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚